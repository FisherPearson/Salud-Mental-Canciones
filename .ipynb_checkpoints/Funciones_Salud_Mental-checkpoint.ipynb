{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Manejo de Informacion\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Textos\"\"\"\n",
    "\n",
    "import re \n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "\"\"\"Visualizaciones\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\"\"\"ML\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,15)\n",
    "plt.style.use('seaborn-poster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_transform(df):\n",
    "    \"\"\"\n",
    "    Función que realiza la limpieza básica de tu documento original.\n",
    "    \n",
    "    df: documento csv que crearte con el script API_datos.ipynb\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[~(df[\"lyrics\"] == \"error\")] # No tomar en cuenta canciones que no tienen letra\n",
    "    \n",
    "    df[\"lyrics\"] = df.lyrics.str.replace(\"\\r\",\" \").str.replace(\"\\n\",\" \").str.replace(\"[}{&:;,.¡!¿?\\(\\)\\-\\\"\\\"0-9]\",\"\").str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.lower() # Quitar espacios, interlineados, reemplazar algunos signos/numeros y pasar a minúsculas.\n",
    "    \n",
    "    df[\"lyrics\"] = df.lyrics.apply(lambda x: unidecode(x)) # Quitar unicodes de la forma \\uxxxx\n",
    "    \n",
    "    df[\"lyrics\"] = df.lyrics.apply(lambda x: \" \".join(x.split())) # Strippear el texto (quitar espacios innecesarios)\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener Datos\n",
    "\n",
    "Se tienen 3 datasets disponibles por si no se quiere/puede obtener la información personal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = CSV_transform(pd.read_csv(\"top_david_spotify.csv\",usecols = [\"name\",\"lyrics\"]).dropna().reset_index(drop = True))\n",
    "df2 = CSV_transform(pd.read_csv(\"top_javier_spotify.csv\",usecols = [\"name\",\"lyrics\"]).dropna().reset_index(drop = True))\n",
    "df3 = CSV_transform(pd.read_csv(\"top_jesus_spotify.csv\",usecols = [\"name\",\"lyrics\"]).dropna().reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obtener_datos(numero = 1):\n",
    "    \"\"\"\n",
    "    Función para obtener datos en caso de no poder acceder a tus propios datos. Se pone a disposición 3 datasets correspondientes a los csv de 3 personas diferentes.\n",
    "    \n",
    "    numero (1 default): 1: Documento 1 con Inglés, Francés y Español, 2: Documento 2 con Inglés, Francés y Español, 3: Documento 3 con Inglés y Francés. \n",
    "    \"\"\"\n",
    "    \n",
    "    if numero == 1:\n",
    "        return df1\n",
    "    if numero == 2:\n",
    "        return df2\n",
    "    if numero == 3:\n",
    "        return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener datos Estadísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_e = CSV_transform(pd.read_csv(\"top_david_spotify.csv\",usecols = [\"name\",\"lyrics\",\"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]).dropna().reset_index(drop = True)).drop(columns = \"lyrics\")\n",
    "df2_e = CSV_transform(pd.read_csv(\"top_javier_spotify.csv\",usecols = [\"name\",\"lyrics\",\"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]).dropna().reset_index(drop = True)).drop(columns = \"lyrics\")\n",
    "df3_e = CSV_transform(pd.read_csv(\"top_jesus_spotify.csv\",usecols = [\"name\",\"lyrics\",\"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]).dropna().reset_index(drop = True)).drop(columns = \"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obtener_datos_estadisticos(numero = 1):\n",
    "    \"\"\"\n",
    "    Función para obtener datos ESTADÍSTICOS en caso de no poder acceder a tus propios datos. Se pone a disposición 3 datasets correspondientes a los csv de 3 personas diferentes. Los datos son métricas Spotify conformadas por valores continuos y nominales. \n",
    "    \n",
    "    numero: 1: Documento 1 con Inglés, Francés y Español, 2: Documento 2 con Inglés, Francés y Español, 3: Documento 3 con Inglés y Francés. \n",
    "    \"\"\"\n",
    "    \n",
    "    if numero == 1:\n",
    "        return df1_e\n",
    "    if numero == 2:\n",
    "        return df2_e\n",
    "    if numero == 3:\n",
    "        return df3_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier para Identificar idioma\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 13: clasificador de Lengua (Naïve Bayes)__\n",
    "2. __Tema 4: Matriz de Incidencia (frecuencias)__\n",
    "\n",
    "Entrenar algorítmo de clasificación para clasificar entre 17 lenguas. Se utilizará el algorítmo visto en clase, pero implementado por Sklearn.\n",
    "\n",
    "[El conjunto de datos etiquetado](https://www.kaggle.com/datasets/basilb2s/language-detection) fue extraido de Kaggle para facilitar el etiquetado. \n",
    "\n",
    "1) English\n",
    "2) Malayalam\n",
    "3) Hindi\n",
    "4) Tamil\n",
    "5) Kannada\n",
    "6) French\n",
    "7) Spanish\n",
    "8) Portuguese\n",
    "9) Italian\n",
    "10) Russian\n",
    "11) Sweedish\n",
    "12) Dutch\n",
    "13) Arabic\n",
    "14) Turkish\n",
    "15) German\n",
    "16) Danish\n",
    "17) Greek\n",
    "\n",
    "__NOTA__: Dado que la longitud de las canciones no es tan extensa, no se aplicará ningún tipo de stemming. Tampoco considero necesario aplicar la técnica de los bigramas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar Dataset\n",
    "lenguajes = pd.read_csv(\"Language Detection.csv\")\n",
    "\n",
    "# Realizar Matriz de Incidencias\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lenguajes.Text.str.replace(\"[{}:;,.¡!¿?\\(\\)\\\"\\\"0-9]\",\"\").to_list())\n",
    "\n",
    "# Crear modelo \n",
    "\n",
    "NB = MultinomialNB() # Dejar prior como uniforme\n",
    "NB.fit(X, lenguajes.Language.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identificar_Idioma(df):\n",
    "    \"\"\"\n",
    "    Modelo Naive Bayes pre-entrenado para detectar el lenguaje de un texto. Actualmente identifica 17 idiomas.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_test  = vectorizer.transform(df.lyrics)\n",
    "    \n",
    "    df[\"Idioma\"] = NB.predict(X_test)\n",
    "    \n",
    "    return \"Exitoso Identificador de Idioma\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTA:__ La evaluación del modelo está presentado en Identificador_Salud_Mental "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completado de palabras en español en caso de contracción\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 6: Levenshtein Metric__\n",
    "\n",
    "En español, como bien sabemos, no existen formalmente las contracciones; sin embargo, pragmáticamente se ha adquirido la costumbre de \"recortar\" algunas palabras y la forma de representar este fenómeno es por medio de un __'__. Comunmente estas contracciones se efectuan __en preposiciones__, esto es, stopwords. En cuanto a RI no son relevantes, pero para la interpretación literaria del texto, sí.\n",
    "\n",
    "Se pretende identificar estas palabras y completarlas por medio de una lista de palabras comunmente contraidas (informalmente) en el español. De no detectarse alguna candidata, es mejor elimina la palabra por cuestiones de carácteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contracciones_español(df,porcentaje = 60, lista_extra = []): \n",
    "    \"\"\"\n",
    "    Función para corregir contracciones en español denotadas por \" ' \" por medio de la distancia de Leveinshtein. \n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    porcentaje (60 default): porcentaje al cual se desea manejar la similitud entre las correcciones y las palabras contraidas.\n",
    "    \n",
    "    lista_extra ([] default): lista de palabras anexadas a la lista ya implementada (stopwords) en la función. \n",
    "    \"\"\"\n",
    "    \n",
    "    assert \"Spanish\" in df[\"Idioma\"].unique(), 'No escuchas música en español'\n",
    "    \n",
    "    lista = lista_extra + [palabra for palabra in stopwords.words(\"Spanish\") if len(palabra) >=3] # limpia las palabras menores a 3 carácteres en la lista\n",
    "    \n",
    "    español = df[df[\"Idioma\"] == \"Spanish\"][\"lyrics\"]\n",
    "    \n",
    "    for indice in español.index:\n",
    "            palabras_cancion = español[indice].split()\n",
    "            \n",
    "            for index in range(len(palabras_cancion)):\n",
    "                if \"'\" in palabras_cancion[index]:\n",
    "                    try:\n",
    "                        palabras_cancion[index] =  process.extractOne(palabras_cancion[index], lista,score_cutoff = porcentaje)[0] # Definir el mejor candidato a remplazo de las palabras con contracción\n",
    "                    except:\n",
    "                        palabras_cancion[index] = \"\" # De no encontrar candidato, eliminala. \n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            cancion_corregida = \" \".join(palabras_cancion)\n",
    "            \n",
    "            df.iloc[indice,1] = cancion_corregida\n",
    "    \n",
    "    return \"Exitosa corrección de palabras en la lista\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contracciones general(')\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 3: Regex (Expresiones regulares)__\n",
    "\n",
    "Dado que en algunas lenguas romances el uso de __'__ resulta determinante para el contexto de la oración, no puede ser fácilmente eliminado del corpus. En adición, las contracciones en el idioma inglés también existen y son muy comunes.  En general, en caso de que la contracción sea entre una preposición y una palabra relevante, es más probable que la palabra sea de mediana longitud. La función está primordialmente orientada a lenguas romance (incluyendo inglés) que las utilicen.\n",
    "\n",
    "Se pretende identificar contracciones útiles por medio de la identificación de la longitud de la segunda palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contracciones_general(df, limite = 4):\n",
    "    \"\"\"\n",
    "    Función para eliminar contracciones en otros idiomas, comunmente lenguas romance, y que solo toma en cuenta las palabras con el número de carácteres mayor o igual al establecido en el límite.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    limite: límite establecido\n",
    "    \n",
    "    Ejemplo: (Límite 5) La función acepta l'internet --> internet; la función no acepta l'étrê -/-> étrê por ser menor a 5.\n",
    "    \"\"\"\n",
    "    \n",
    "    idiomas = df[~(df[\"Idioma\"] == \"Spanish\")][\"lyrics\"]\n",
    "    \n",
    "    for indice in idiomas.index:\n",
    "            palabras_cancion = idiomas[indice].split()\n",
    "            \n",
    "            for index in range(len(palabras_cancion)):\n",
    "                if \"'\" in palabras_cancion[index]:\n",
    "                    \n",
    "                    try:\n",
    "                        palabras_cancion[index] =  re.search(\"(?<=')\\w{4,}\",palabras_cancion[index])[0] # Expresión para tomar en cuenta las cadenas que precedan un ' y tengan longitud mayor a 4\n",
    "                    except:\n",
    "                        palabras_cancion[index] = \"\"\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            cancion_corregida = \" \".join(palabras_cancion)\n",
    "            \n",
    "            df.iloc[indice,1] = cancion_corregida\n",
    "    \n",
    "    return \"Exitosas correcciones generales de contracciones en la lista\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizar Canciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizar(df):\n",
    "    \"\"\"\n",
    "    Tokeniza cada una de las canciones. \n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"tokens\"] = df[\"lyrics\"].apply(lambda x: set(nltk.word_tokenize(x))) # Tokenizar las canciones\n",
    "    \n",
    "    return \"Exitoso Tokenizado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar palabras de longitud mayor a..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eliminar_mayor_len(df,limite = 10):\n",
    "    \"\"\"\n",
    "    Función que elimina las palabras con longitud mayor al límite establecido.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    limite: límite establecido.\n",
    "    \"\"\"\n",
    "    \n",
    "    limpias = []\n",
    "    for i in range(df.shape[0]):\n",
    "        limpias.append({palabra_menos for palabra_menos in df.tokens[i] if len(palabra_menos) < limite})\n",
    "    \n",
    "    df[\"tokens\"] = limpias\n",
    "    \n",
    "    return f\"Exitosa eliminación de palabras con longitud mayor a {limite}\"\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar palabras de longitud menor a..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eliminar_menor_len(df,limite = 3):\n",
    "    \"\"\"\n",
    "    Función que elimina las palabras con longitud menor al límite establecido.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    limite: límite establecido.\n",
    "    \"\"\"\n",
    "    limpias = []\n",
    "    for i in range(df.shape[0]):\n",
    "        limpias.append({palabra_menos for palabra_menos in df.tokens[i] if len(palabra_menos) > limite})\n",
    "    \n",
    "    df[\"tokens\"] = limpias\n",
    "    \n",
    "    return f\"Exitosa eliminación de palabras con longitud menor a {limite}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stopwords(df, lista_extra = []):\n",
    "    # Lista de idiomas\n",
    "    idiomas = df.Idioma.unique()\n",
    "    \n",
    "    \n",
    "    for idioma in idiomas:\n",
    "        try:\n",
    "            stopwords_ = stopwords.words(idioma) + lista_extra # Stopwords por idioma y se incluyen la lista de palabras extra\n",
    "        except:\n",
    "            print(\"No hay stopwords para\", idioma)\n",
    "            \n",
    "        canciones_idioma = df[df[\"Idioma\"] == idioma][\"tokens\"]\n",
    "        \n",
    "        for indice in canciones_idioma.index:\n",
    "            canciones_idioma[indice] = [palabra for palabra in canciones_idioma[indice] if palabra not in stopwords_] # elimina las palabras que no requiere en análsis.\n",
    "            \n",
    "        df.iloc[canciones_idioma.index,3] = canciones_idioma\n",
    "        \n",
    "    return \"Exitosa eliminación de stopwords por idiomas identificados\"\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds por idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordClouds_idiomas(df):\n",
    "    \"\"\"\n",
    "    Función que implime por idioma una representación \"Nube de Palabras\".\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \"\"\"\n",
    "    for idioma in df[\"Idioma\"].unique():\n",
    "    \n",
    "        freq = \" \".join(list(chain(*df[df[\"Idioma\"] == idioma][\"tokens\"])))\n",
    "        cloud = WordCloud(width = 8000,height = 8000, background_color = \"black\",max_words = 80).generate(freq)\n",
    "    \n",
    "        plt.figure(figsize=(25,15))\n",
    "        plt.imshow(cloud, interpolation = \"bilinear\")\n",
    "        plt.title(idioma, fontsize = 20, color = \"black\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmizar por idioma\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Stemming__\n",
    "\n",
    "\n",
    "La función solo está disponible para los siguientes idiomas:\n",
    "\n",
    "1. arabic\n",
    "2. danish\n",
    "3. dutch\n",
    "4. english\n",
    "5. finnish\n",
    "6. french\n",
    "7. german\n",
    "8. hungarian\n",
    "9. italian\n",
    "10. norwegian\n",
    "11. portuguese\n",
    "12. romanian\n",
    "13. russian\n",
    "14. spanish\n",
    "15. swedish\n",
    "\n",
    "De no estar disponible el idioma, simplemente se omite y se dejan los tokens originales. Se optó por la opción de Stemming por el simple hecho de que existía el método ya prehecho para varios idiomas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_idiomas(df):\n",
    "    \"\"\"\n",
    "    Función que stemmiza las canciones dependiendo del idioma identificado.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"tokens_stem\"] = np.nan\n",
    "    for idioma in df.Idioma.unique():\n",
    "        \n",
    "        try:\n",
    "            stemmer = SnowballStemmer(idioma.lower())\n",
    "            print(idioma, \" ¡disponible para stemmizar!\") # Avisar qué lenguajes están disponibles.\n",
    "        except:\n",
    "            continue # Si no se encuentra el idioma del cual se quieren estemizar las canciones, solamente se ignoran y se dejan como originalmente están.\n",
    "        \n",
    "        canciones_idioma = df[df[\"Idioma\"] == idioma][\"tokens\"]\n",
    "        \n",
    "        for indice in canciones_idioma.index:\n",
    "            canciones_idioma[indice] = [stemmer.stem(palabra) for palabra in canciones_idioma[indice]] # Función stemming por idioma. \n",
    "            \n",
    "        df.iloc[canciones_idioma.index,4] = canciones_idioma\n",
    "        \n",
    "    return \"Exitosa eliminación de stopwords por idiomas identificados\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar palabras clave por (entre) canción(es)\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 8: TF-IDF__\n",
    "\n",
    "Se busca obtener una aproximación al contexto de las canciones por medio del TF-IDF. Se pretende encontrar canciones con contextos parecidos. Se separa por idiomas para una mejor visualización, aunque ciertamente podría no hacerse y aún con eso el algortimo seguiría funcionando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_mas_representativas_idioma(df, idioma = \"Spanish\"):\n",
    "    \"\"\"\n",
    "    Función que calcula la relevancia de las palabras por canción por medio del cálculo de su TF-IDF\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    idioma: Nombre del idioma de las canciones que se quiere analizar. Inicia con mayúsculas y en inglés.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(df[df[\"Idioma\"] == idioma].tokens_stem.apply(lambda x: \" \".join(x)).tolist()) # Filtrar por idioma\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    tfidf = pd.DataFrame(denselist, columns=feature_names, index = df[df[\"Idioma\"] == idioma].name)\n",
    "    \n",
    "    \n",
    "    tuplas = []\n",
    "    for i in range(len(tfidf.index)):\n",
    "        cancion = tfidf.iloc[i,].where(lambda x: x >0).dropna().sort_values(ascending = False) # Calcular el TF-IDF más grande de cada idioma, así como proveer las palabras a los cuales corresponden los valores y la canción\n",
    "        tuplas.append(list(cancion[cancion == cancion.max()].index))\n",
    "        \n",
    "    return dict(zip(tfidf.index,tuplas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering respecto a Métricas Spotify\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __PCA__\n",
    "\n",
    "[El conjunto de datos](https://www.kaggle.com/datasets/geomack/spotifyclassification) fue extraido de kaggle para tener mayor variedad en las canciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"kmeans_train.csv\", usecols = [\"song_title\", \"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"])\n",
    "datos = datos[[\"song_title\", \"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]].rename(columns = {\"song_title\":\"name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mask Off</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.434</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.795</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>0.01020</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Redbone</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.359</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.401</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.19900</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xanny Family</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.412</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.148</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.03440</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Master Of None</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.338</td>\n",
       "      <td>5</td>\n",
       "      <td>-15.236</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.60400</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parallel Lines</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.561</td>\n",
       "      <td>5</td>\n",
       "      <td>-11.648</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.18000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>Like A Bitch - Kill The Noise Remix</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.932</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.501</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>0.00106</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>Candy</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.663</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.08770</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>Habit - Dack Janiels &amp; Wenzday Remix</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.467</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.00857</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>First Contact</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.735</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>I Wanna Get Better</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.915</td>\n",
       "      <td>9</td>\n",
       "      <td>-6.221</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2017 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name  danceability  energy  key  \\\n",
       "0                                 Mask Off         0.833   0.434    2   \n",
       "1                                  Redbone         0.743   0.359    1   \n",
       "2                             Xanny Family         0.838   0.412    2   \n",
       "3                           Master Of None         0.494   0.338    5   \n",
       "4                           Parallel Lines         0.678   0.561    5   \n",
       "...                                    ...           ...     ...  ...   \n",
       "2012   Like A Bitch - Kill The Noise Remix         0.584   0.932    1   \n",
       "2013                                 Candy         0.894   0.892    1   \n",
       "2014  Habit - Dack Janiels & Wenzday Remix         0.637   0.935    0   \n",
       "2015                         First Contact         0.557   0.992    1   \n",
       "2016                    I Wanna Get Better         0.446   0.915    9   \n",
       "\n",
       "      loudness  speechiness  acousticness  instrumentalness  liveness  valence  \n",
       "0       -8.795       0.4310       0.01020          0.021900    0.1650    0.286  \n",
       "1      -10.401       0.0794       0.19900          0.006110    0.1370    0.588  \n",
       "2       -7.148       0.2890       0.03440          0.000234    0.1590    0.173  \n",
       "3      -15.236       0.0261       0.60400          0.510000    0.0922    0.230  \n",
       "4      -11.648       0.0694       0.18000          0.512000    0.4390    0.904  \n",
       "...        ...          ...           ...               ...       ...      ...  \n",
       "2012    -3.501       0.3330       0.00106          0.002690    0.1290    0.211  \n",
       "2013    -2.663       0.1310       0.08770          0.001670    0.0528    0.867  \n",
       "2014    -2.467       0.1070       0.00857          0.003990    0.2140    0.470  \n",
       "2015    -2.735       0.1330       0.00164          0.677000    0.0913    0.623  \n",
       "2016    -6.221       0.1410       0.00281          0.000039    0.2180    0.402  \n",
       "\n",
       "[2017 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de correlaciones\n",
    "\n",
    "No hay variables que se correlaciones de manera severa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>danceability</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066778</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>-0.045479</td>\n",
       "      <td>0.222952</td>\n",
       "      <td>-0.004758</td>\n",
       "      <td>-0.079106</td>\n",
       "      <td>-0.146308</td>\n",
       "      <td>0.435984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>-0.066778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.066907</td>\n",
       "      <td>0.684205</td>\n",
       "      <td>0.232990</td>\n",
       "      <td>-0.496084</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.222124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.066907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>0.027697</td>\n",
       "      <td>-0.090925</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.055685</td>\n",
       "      <td>0.030163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loudness</th>\n",
       "      <td>-0.045479</td>\n",
       "      <td>0.684205</td>\n",
       "      <td>0.046165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.165905</td>\n",
       "      <td>-0.349036</td>\n",
       "      <td>-0.280059</td>\n",
       "      <td>0.172602</td>\n",
       "      <td>0.122458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speechiness</th>\n",
       "      <td>0.222952</td>\n",
       "      <td>0.232990</td>\n",
       "      <td>0.027697</td>\n",
       "      <td>0.165905</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.135673</td>\n",
       "      <td>-0.137574</td>\n",
       "      <td>0.094048</td>\n",
       "      <td>0.094036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acousticness</th>\n",
       "      <td>-0.004758</td>\n",
       "      <td>-0.496084</td>\n",
       "      <td>-0.090925</td>\n",
       "      <td>-0.349036</td>\n",
       "      <td>-0.135673</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.128579</td>\n",
       "      <td>-0.077612</td>\n",
       "      <td>0.016429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentalness</th>\n",
       "      <td>-0.079106</td>\n",
       "      <td>0.023661</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>-0.280059</td>\n",
       "      <td>-0.137574</td>\n",
       "      <td>-0.128579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>-0.165115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveness</th>\n",
       "      <td>-0.146308</td>\n",
       "      <td>0.206107</td>\n",
       "      <td>0.055685</td>\n",
       "      <td>0.172602</td>\n",
       "      <td>0.094048</td>\n",
       "      <td>-0.077612</td>\n",
       "      <td>-0.002252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.435984</td>\n",
       "      <td>0.222124</td>\n",
       "      <td>0.030163</td>\n",
       "      <td>0.122458</td>\n",
       "      <td>0.094036</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>-0.165115</td>\n",
       "      <td>-0.084900</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  danceability    energy       key  loudness  speechiness  \\\n",
       "danceability          1.000000 -0.066778  0.022812 -0.045479     0.222952   \n",
       "energy               -0.066778  1.000000  0.066907  0.684205     0.232990   \n",
       "key                   0.022812  0.066907  1.000000  0.046165     0.027697   \n",
       "loudness             -0.045479  0.684205  0.046165  1.000000     0.165905   \n",
       "speechiness           0.222952  0.232990  0.027697  0.165905     1.000000   \n",
       "acousticness         -0.004758 -0.496084 -0.090925 -0.349036    -0.135673   \n",
       "instrumentalness     -0.079106  0.023661  0.000368 -0.280059    -0.137574   \n",
       "liveness             -0.146308  0.206107  0.055685  0.172602     0.094048   \n",
       "valence               0.435984  0.222124  0.030163  0.122458     0.094036   \n",
       "\n",
       "                  acousticness  instrumentalness  liveness   valence  \n",
       "danceability         -0.004758         -0.079106 -0.146308  0.435984  \n",
       "energy               -0.496084          0.023661  0.206107  0.222124  \n",
       "key                  -0.090925          0.000368  0.055685  0.030163  \n",
       "loudness             -0.349036         -0.280059  0.172602  0.122458  \n",
       "speechiness          -0.135673         -0.137574  0.094048  0.094036  \n",
       "acousticness          1.000000         -0.128579 -0.077612  0.016429  \n",
       "instrumentalness     -0.128579          1.000000 -0.002252 -0.165115  \n",
       "liveness             -0.077612         -0.002252  1.000000 -0.084900  \n",
       "valence               0.016429         -0.165115 -0.084900  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.corr(\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto `key` como `loudness` tienen una varianza mayor. Es necesario escalarlas. Las demás variables ya se encuentran reescaladas, pero podría disminuirla la desviación estandar un poco más en aras de una mejor captura de los componentes principales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.618422</td>\n",
       "      <td>0.681577</td>\n",
       "      <td>5.342588</td>\n",
       "      <td>-7.085624</td>\n",
       "      <td>0.092664</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.133286</td>\n",
       "      <td>0.190844</td>\n",
       "      <td>0.496815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161029</td>\n",
       "      <td>0.210273</td>\n",
       "      <td>3.648240</td>\n",
       "      <td>3.761684</td>\n",
       "      <td>0.089931</td>\n",
       "      <td>0.259989</td>\n",
       "      <td>0.273162</td>\n",
       "      <td>0.155453</td>\n",
       "      <td>0.247195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-33.097000</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-8.394000</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-6.248000</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-4.746000</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-0.307000</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       danceability       energy          key     loudness  speechiness  \\\n",
       "count   2017.000000  2017.000000  2017.000000  2017.000000  2017.000000   \n",
       "mean       0.618422     0.681577     5.342588    -7.085624     0.092664   \n",
       "std        0.161029     0.210273     3.648240     3.761684     0.089931   \n",
       "min        0.122000     0.014800     0.000000   -33.097000     0.023100   \n",
       "25%        0.514000     0.563000     2.000000    -8.394000     0.037500   \n",
       "50%        0.631000     0.715000     6.000000    -6.248000     0.054900   \n",
       "75%        0.738000     0.846000     9.000000    -4.746000     0.108000   \n",
       "max        0.984000     0.998000    11.000000    -0.307000     0.816000   \n",
       "\n",
       "       acousticness  instrumentalness     liveness      valence  \n",
       "count   2017.000000       2017.000000  2017.000000  2017.000000  \n",
       "mean       0.187590          0.133286     0.190844     0.496815  \n",
       "std        0.259989          0.273162     0.155453     0.247195  \n",
       "min        0.000003          0.000000     0.018800     0.034800  \n",
       "25%        0.009630          0.000000     0.092300     0.295000  \n",
       "50%        0.063300          0.000076     0.127000     0.492000  \n",
       "75%        0.265000          0.054000     0.247000     0.691000  \n",
       "max        0.995000          0.976000     0.969000     0.992000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos[['key', 'loudness']] = StandardScaler().fit_transform(datos[['key', 'loudness']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mask Off</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.434</td>\n",
       "      <td>-0.916446</td>\n",
       "      <td>-0.454530</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>0.01020</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Redbone</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.359</td>\n",
       "      <td>-1.190619</td>\n",
       "      <td>-0.881573</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.19900</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.1370</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xanny Family</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.412</td>\n",
       "      <td>-0.916446</td>\n",
       "      <td>-0.016586</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.03440</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Master Of None</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.093928</td>\n",
       "      <td>-2.167220</td>\n",
       "      <td>0.0261</td>\n",
       "      <td>0.60400</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Parallel Lines</td>\n",
       "      <td>0.678</td>\n",
       "      <td>0.561</td>\n",
       "      <td>-0.093928</td>\n",
       "      <td>-1.213155</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.18000</td>\n",
       "      <td>0.512000</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>Like A Bitch - Kill The Noise Remix</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.932</td>\n",
       "      <td>-1.190619</td>\n",
       "      <td>0.953167</td>\n",
       "      <td>0.3330</td>\n",
       "      <td>0.00106</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>Candy</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.892</td>\n",
       "      <td>-1.190619</td>\n",
       "      <td>1.175995</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.08770</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>Habit - Dack Janiels &amp; Wenzday Remix</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.935</td>\n",
       "      <td>-1.464792</td>\n",
       "      <td>1.228112</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.00857</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>First Contact</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.992</td>\n",
       "      <td>-1.190619</td>\n",
       "      <td>1.156850</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>I Wanna Get Better</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.915</td>\n",
       "      <td>1.002763</td>\n",
       "      <td>0.229907</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2017 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name  danceability  energy       key  \\\n",
       "0                                 Mask Off         0.833   0.434 -0.916446   \n",
       "1                                  Redbone         0.743   0.359 -1.190619   \n",
       "2                             Xanny Family         0.838   0.412 -0.916446   \n",
       "3                           Master Of None         0.494   0.338 -0.093928   \n",
       "4                           Parallel Lines         0.678   0.561 -0.093928   \n",
       "...                                    ...           ...     ...       ...   \n",
       "2012   Like A Bitch - Kill The Noise Remix         0.584   0.932 -1.190619   \n",
       "2013                                 Candy         0.894   0.892 -1.190619   \n",
       "2014  Habit - Dack Janiels & Wenzday Remix         0.637   0.935 -1.464792   \n",
       "2015                         First Contact         0.557   0.992 -1.190619   \n",
       "2016                    I Wanna Get Better         0.446   0.915  1.002763   \n",
       "\n",
       "      loudness  speechiness  acousticness  instrumentalness  liveness  valence  \n",
       "0    -0.454530       0.4310       0.01020          0.021900    0.1650    0.286  \n",
       "1    -0.881573       0.0794       0.19900          0.006110    0.1370    0.588  \n",
       "2    -0.016586       0.2890       0.03440          0.000234    0.1590    0.173  \n",
       "3    -2.167220       0.0261       0.60400          0.510000    0.0922    0.230  \n",
       "4    -1.213155       0.0694       0.18000          0.512000    0.4390    0.904  \n",
       "...        ...          ...           ...               ...       ...      ...  \n",
       "2012  0.953167       0.3330       0.00106          0.002690    0.1290    0.211  \n",
       "2013  1.175995       0.1310       0.08770          0.001670    0.0528    0.867  \n",
       "2014  1.228112       0.1070       0.00857          0.003990    0.2140    0.470  \n",
       "2015  1.156850       0.1330       0.00164          0.677000    0.0913    0.623  \n",
       "2016  0.229907       0.1410       0.00281          0.000039    0.2180    0.402  \n",
       "\n",
       "[2017 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.618422</td>\n",
       "      <td>0.681577</td>\n",
       "      <td>7.166635e-17</td>\n",
       "      <td>2.554834e-16</td>\n",
       "      <td>0.092664</td>\n",
       "      <td>0.187590</td>\n",
       "      <td>0.133286</td>\n",
       "      <td>0.190844</td>\n",
       "      <td>0.496815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161029</td>\n",
       "      <td>0.210273</td>\n",
       "      <td>1.000248e+00</td>\n",
       "      <td>1.000248e+00</td>\n",
       "      <td>0.089931</td>\n",
       "      <td>0.259989</td>\n",
       "      <td>0.273162</td>\n",
       "      <td>0.155453</td>\n",
       "      <td>0.247195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>-1.464792e+00</td>\n",
       "      <td>-6.916536e+00</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.563000</td>\n",
       "      <td>-9.164464e-01</td>\n",
       "      <td>-3.479027e-01</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.631000</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>1.802444e-01</td>\n",
       "      <td>2.227279e-01</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.738000</td>\n",
       "      <td>0.846000</td>\n",
       "      <td>1.002763e+00</td>\n",
       "      <td>6.221161e-01</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>1.551108e+00</td>\n",
       "      <td>1.802465e+00</td>\n",
       "      <td>0.816000</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.969000</td>\n",
       "      <td>0.992000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       danceability       energy           key      loudness  speechiness  \\\n",
       "count   2017.000000  2017.000000  2.017000e+03  2.017000e+03  2017.000000   \n",
       "mean       0.618422     0.681577  7.166635e-17  2.554834e-16     0.092664   \n",
       "std        0.161029     0.210273  1.000248e+00  1.000248e+00     0.089931   \n",
       "min        0.122000     0.014800 -1.464792e+00 -6.916536e+00     0.023100   \n",
       "25%        0.514000     0.563000 -9.164464e-01 -3.479027e-01     0.037500   \n",
       "50%        0.631000     0.715000  1.802444e-01  2.227279e-01     0.054900   \n",
       "75%        0.738000     0.846000  1.002763e+00  6.221161e-01     0.108000   \n",
       "max        0.984000     0.998000  1.551108e+00  1.802465e+00     0.816000   \n",
       "\n",
       "       acousticness  instrumentalness     liveness      valence  \n",
       "count   2017.000000       2017.000000  2017.000000  2017.000000  \n",
       "mean       0.187590          0.133286     0.190844     0.496815  \n",
       "std        0.259989          0.273162     0.155453     0.247195  \n",
       "min        0.000003          0.000000     0.018800     0.034800  \n",
       "25%        0.009630          0.000000     0.092300     0.295000  \n",
       "50%        0.063300          0.000076     0.127000     0.492000  \n",
       "75%        0.265000          0.054000     0.247000     0.691000  \n",
       "max        0.995000          0.976000     0.969000     0.992000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datos.iloc[:,1:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2) \n",
    "  \n",
    "X_pca = pca.fit_transform(X) \n",
    " \n",
    "explained_variance = pca.explained_variance_ratio_ \n",
    "\n",
    "principalDf = pd.DataFrame(data = X_pca, columns = ['Componente1', 'Componente2'])\n",
    "\n",
    "# sns.barplot(x = [\"Primer Componente\",\"Segundo Componente\"], y = explained_variance)\n",
    "# plt.title(\"Varianza Explicada 90%\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La curva del codo sugiere que 4 clusters son los indispensables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nc = range(1, 10)\n",
    "# kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "# kmeans\n",
    "# score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]\n",
    "# score\n",
    "# plt.plot(Nc,score)\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Elbow Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La gráfica de silueta también da buenos resultados con 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tomado de: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "# range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "# for n_clusters in range_n_clusters:\n",
    "#     # Create a subplot with 1 row and 2 columns\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     fig.set_size_inches(18, 7)\n",
    "\n",
    "#     # The 1st subplot is the silhouette plot\n",
    "#     # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "#     # lie within [-0.1, 1]\n",
    "#     ax1.set_xlim([-0.1, 1])\n",
    "#     # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "#     # plots of individual clusters, to demarcate them clearly.\n",
    "#     ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "#     # Initialize the clusterer with n_clusters value and a random generator\n",
    "#     # seed of 10 for reproducibility.\n",
    "#     clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "#     cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "#     # The silhouette_score gives the average value for all the samples.\n",
    "#     # This gives a perspective into the density and separation of the formed\n",
    "#     # clusters\n",
    "#     silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "#     print(\n",
    "#         \"For n_clusters =\",\n",
    "#         n_clusters,\n",
    "#         \"The average silhouette_score is :\",\n",
    "#         silhouette_avg,\n",
    "#     )\n",
    "\n",
    "#     # Compute the silhouette scores for each sample\n",
    "#     sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "#     y_lower = 10\n",
    "#     for i in range(n_clusters):\n",
    "#         # Aggregate the silhouette scores for samples belonging to\n",
    "#         # cluster i, and sort them\n",
    "#         ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "#         ith_cluster_silhouette_values.sort()\n",
    "\n",
    "#         size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "#         y_upper = y_lower + size_cluster_i\n",
    "\n",
    "#         color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "#         ax1.fill_betweenx(\n",
    "#             np.arange(y_lower, y_upper),\n",
    "#             0,\n",
    "#             ith_cluster_silhouette_values,\n",
    "#             facecolor=color,\n",
    "#             edgecolor=color,\n",
    "#             alpha=0.7,\n",
    "#         )\n",
    "\n",
    "#         # Label the silhouette plots with their cluster numbers at the middle\n",
    "#         ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "#         # Compute the new y_lower for next plot\n",
    "#         y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "#     ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "#     ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "#     ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "#     # The vertical line for average silhouette score of all the values\n",
    "#     ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "#     ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "#     ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "#     # 2nd Plot showing the actual clusters formed\n",
    "#     colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "#     ax2.scatter(\n",
    "#         X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "#     )\n",
    "\n",
    "#     # Labeling the clusters\n",
    "#     centers = clusterer.cluster_centers_\n",
    "#     # Draw white circles at cluster centers\n",
    "#     ax2.scatter(\n",
    "#         centers[:, 0],\n",
    "#         centers[:, 1],\n",
    "#         marker=\"o\",\n",
    "#         c=\"white\",\n",
    "#         alpha=1,\n",
    "#         s=200,\n",
    "#         edgecolor=\"k\",\n",
    "#     )\n",
    "\n",
    "#     for i, c in enumerate(centers):\n",
    "#         ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "#     ax2.set_title(\"The visualization of the clustered data.\")\n",
    "#     ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "#     ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "#     plt.suptitle(\n",
    "#         \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "#         % n_clusters,\n",
    "#         fontsize=14,\n",
    "#         fontweight=\"bold\",\n",
    "#     )\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=8).fit(X)\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "datos[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "0    427\n",
       "1    725\n",
       "2    752\n",
       "3    113\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.groupby(\"labels\")[\"name\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.DataFrame(X)\n",
    "# a[\"labels\"] = labels\n",
    "# sns.scatterplot(data=a, x=0, y=1, hue = \"labels\",palette=\"deep\")\n",
    "# plt.xlabel(\"Primer Componente\")\n",
    "# plt.ylabel(\"Segundo Componente\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción por clusters\n",
    "\n",
    "1. __Grupo 0 (a toda madre)__: Canciones llenas de Energía, te dan ganas de bailarlas.\n",
    "\n",
    "2. __Grupo 1 (fuera de este mundo)__: Pasas el rato y escuchar música te saca de este mundo. \n",
    "\n",
    "3. __Grupo 2 (cansado, escuchando música porque andas aburrido)__: Canciones energéticas, pero no tan ruidosas. Canciones para concentrarse.\n",
    "\n",
    "4. __Grupo 3 (¿Todo bien, bro?)__: Canciones en su mayoría con una valencia baja y muy poco energéticas. En su mayoría son canciones acústicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cluster_canciones(df, test_e):\n",
    "    \"\"\"\n",
    "    Algorítmo de clusterización que clasifica las canciones en 4 grupos.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    test_e: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos_estadisticos.\n",
    "    \"\"\"\n",
    "    \n",
    "    test_e[['key', 'loudness']] = StandardScaler().fit_transform(test_e[['key', 'loudness']])\n",
    "    \n",
    "    X = test_e.iloc[:,1:].values\n",
    "    \n",
    "    X_pca = pca.transform(X) \n",
    "    \n",
    "    X = np.array(X_pca)\n",
    "    \n",
    "    labels = kmeans.predict(X)\n",
    "    \n",
    "    df[\"Tipo\"] = labels\n",
    "    \n",
    "    df[\"Tipo\"] = df[\"Tipo\"].replace(0,\"A toda madre\").replace(1,\"fuera de este mundo\").replace(2,\"cansado/aburrido\").replace(3,\"¿Todo bien,bro?\")\n",
    "    \n",
    "    return \"¡Exitoso etiquetado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilidad de estar triste dado el número de canciones tristes\n",
    "\n",
    "Método frecuentista de proporciones que estima la probabilidad (proporción) de haber tenido algún estado de ánimo atrás descrito. Conteo de $\\frac{\\text{# Existos}}{Total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estoy_triste_o_no(df):\n",
    "    \"\"\"\n",
    "    Función que provee un resumen probabilístico de qué estado de ánimo estuvo más presente a lo largo del último mes de la persona en cuestión.\n",
    "    \n",
    "    df: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Consideremos el número de canciones tristes que reproduciste en tu mes:\")\n",
    "    \n",
    "    print(df.groupby(\"Tipo\")[\"name\"].count(), 2*\"\\n\")\n",
    "    \n",
    "    print(\"Esto significa que la proporción de canciones tristes fue: \", 100*round(df[df.Tipo == \"¿Todo bien,bro?\"].shape[0]/df.shape[0],4),\"%\")\n",
    "    \n",
    "    \n",
    "    labels = list(df.Tipo.value_counts().index)\n",
    "    data = df.Tipo.value_counts(normalize = True).values\n",
    "\n",
    "\n",
    "    colors = sns.color_palette('pastel')[0:5]\n",
    "\n",
    "\n",
    "    plt.pie(data, labels = labels, colors = colors, autopct='%.0f%%')\n",
    "    plt.show()\n",
    "    \n",
    "    sentimiento = df.Tipo.value_counts()[df.Tipo.value_counts() == df.Tipo.value_counts().max()].index[0]\n",
    "    \n",
    "    print(\"Estimo que en este último mes tú te sentiste, en la mayor parte del tiempo,\", sentimiento, 2* \"\\n\")\n",
    "    \n",
    "    if sentimiento == \"A toda madre\":\n",
    "        print(\"Esto significa que estuviste muy feliz la mayor parte del tiempo, lo que ocasionó que escucharas música movida y energética\")\n",
    "    if sentimiento == \"cansado/aburrido\":\n",
    "        print(\"No te sentiste ni muy triste ni muy feliz, más bien neutral. Eso no evitó que siguieras escuchando música tanto movida como algo más tranquilo.\")\n",
    "    if sentimiento == \"fuera de este mundo\":\n",
    "        print(\"La mayor parte del tiempo quisiste vivir desconectado del mundo: canciones tranquilas y una que otra movida fueron las que más predominaron\")\n",
    "    if sentimiento == \"¿Todo bien,bro?\":\n",
    "        print(\"Tristeza/Depresion.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master(test,test_e, limite_mas = 10, limite_menos = 3, stopwords = []):\n",
    "    \"\"\"\n",
    "    Función que realiza todas las funciones básicas de limpieza y ordenamiento de los textos.\n",
    "    \n",
    "    test: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos.\n",
    "    \n",
    "    test_e: Documento CSV generado por API_datos.ipynb o por la función Obtener_datos_estadisticos.\n",
    "    \n",
    "    limite_mas: límete establecido para Eliminar_mayor_len.\n",
    "    \n",
    "    limite_menos: límite establecido para Eliminar_menor_len\n",
    "    \n",
    "    stopwords: lista extra para considerar como stopwords.\n",
    "    \"\"\"\n",
    "    Identificar_Idioma(test)\n",
    "    \n",
    "    try:\n",
    "        Contracciones_español(test)\n",
    "    except AssertionError as msg:\n",
    "        print(msg)\n",
    "\n",
    "    Contracciones_general(test)\n",
    "\n",
    "    Tokenizar(test)\n",
    "\n",
    "    Eliminar_mayor_len(test, limite_mas)\n",
    "\n",
    "    Eliminar_menor_len(test, limite_menos)\n",
    "\n",
    "    Stopwords(test, stopwords) \n",
    "\n",
    "    stemming_idiomas(test)\n",
    "\n",
    "    Cluster_canciones(test,test_e)\n",
    "    \n",
    "    return \"¡Todo listo!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
