{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Manejo de Informacion\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pandas import json_normalize\n",
    "#import requests\n",
    "#import json\n",
    "\n",
    "#\"\"\"Tiempo\"\"\"\n",
    "\n",
    "#from datetime import datetime\n",
    "#from datetime import timezone\n",
    "\n",
    "\"\"\"Textos\"\"\"\n",
    "\n",
    "import re \n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from nltk.stem import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "\"\"\"Visualizaciones\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\"\"\"ML\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSV_transform(df):\n",
    "    \"\"\"\n",
    "    Función que transforma de formato CSV a diccionario\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[~(df[\"lyrics\"] == \"error\")] # No tomar en cuenta canciones que no tienen letra\n",
    "    \n",
    "    df[\"lyrics\"] = df.lyrics.str.replace(\"\\r\",\" \").str.replace(\"\\n\",\" \").str.replace(\"[}{&:;,.¡!¿?\\(\\)\\-\\\"\\\"0-9]\",\"\").str.replace(\"[\",\"\").str.replace(\"]\",\"\").str.lower() # Quitar espacios, interlineados, reemplazar algunos signos/numeros y pasar a minúsculas.\n",
    "    \n",
    "    df[\"lyrics\"] = df.lyrics.apply(lambda x: unidecode(x)) # Quitar unicodes de la forma \\uxxxx\n",
    "    \n",
    "    df[\"lyrics\"] = df.lyrics.apply(lambda x: \" \".join(x.split())) # Strippear el texto (quitar espacios innecesarios)\n",
    "     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener Datos\n",
    "\n",
    "Se tienen 3 datasets disponibles por si no se quiere/puede obtener la información personal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = CSV_transform(pd.read_csv(\"top_david_spotify.csv\",usecols = [\"name\",\"lyrics\"]).dropna().reset_index(drop = True))\n",
    "df2 = CSV_transform(pd.read_csv(\"top_javier_spotify.csv\",usecols = [\"name\",\"lyrics\"]).dropna().reset_index(drop = True))\n",
    "df3 = CSV_transform(pd.read_csv(\"top_jesus_spotify.csv\",usecols = [\"name\",\"lyrics\"]).dropna().reset_index(drop = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obtener_datos(numero):\n",
    "    if numero == 1:\n",
    "        return df1\n",
    "    if numero == 2:\n",
    "        return df2\n",
    "    if numero == 3:\n",
    "        return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener datos Estadísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_e = CSV_transform(pd.read_csv(\"top_david_spotify.csv\",usecols = [\"name\",\"lyrics\",\"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]).dropna().reset_index(drop = True)).drop(columns = \"lyrics\")\n",
    "df2_e = CSV_transform(pd.read_csv(\"top_javier_spotify.csv\",usecols = [\"name\",\"lyrics\",\"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]).dropna().reset_index(drop = True)).drop(columns = \"lyrics\")\n",
    "df3_e = CSV_transform(pd.read_csv(\"top_jesus_spotify.csv\",usecols = [\"name\",\"lyrics\",\"danceability\",\"energy\",\"key\",\"loudness\",\"speechiness\",\"acousticness\",\"instrumentalness\",\"liveness\",\"valence\"]).dropna().reset_index(drop = True)).drop(columns = \"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Obtener_datos_estadisticos(numero):\n",
    "    if numero == 1:\n",
    "        return df1_e\n",
    "    if numero == 2:\n",
    "        return df2_e\n",
    "    if numero == 3:\n",
    "        return df3_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier para Identificar idioma\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 13: clasificador de Lengua (Naïve Bayes)__\n",
    "2. __Tema 4: Matriz de Incidencia (frecuencias)__\n",
    "\n",
    "Entrenar algorítmo de clasificación para clasificar entre 17 lenguas. Se utilizará el algorítmo visto en clase, pero implementado por Sklearn.\n",
    "\n",
    "[El conjunto de datos etiquetado](https://www.kaggle.com/datasets/basilb2s/language-detection) fue extraido de Kaggle para facilitar el etiquetado. \n",
    "\n",
    "1) English\n",
    "2) Malayalam\n",
    "3) Hindi\n",
    "4) Tamil\n",
    "5) Kannada\n",
    "6) French\n",
    "7) Spanish\n",
    "8) Portuguese\n",
    "9) Italian\n",
    "10) Russian\n",
    "11) Sweedish\n",
    "12) Dutch\n",
    "13) Arabic\n",
    "14) Turkish\n",
    "15) German\n",
    "16) Danish\n",
    "17) Greek\n",
    "\n",
    "__NOTA__: Dado que la longitud de las canciones no es tan extensa, no se aplicará ningún tipo de stemming. Tampoco considero necesario aplicar la técnica de los bigramas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar Dataset\n",
    "lenguajes = pd.read_csv(\"Language Detection.csv\")\n",
    "\n",
    "# Realizar Matriz de Incidencias\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(lenguajes.Text.str.replace(\"[{}:;,.¡!¿?\\(\\)\\\"\\\"0-9]\",\"\").to_list())\n",
    "\n",
    "# Crear modelo \n",
    "\n",
    "NB = MultinomialNB() # Dejar prior como uniforme\n",
    "NB.fit(X, lenguajes.Language.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identificar_Idioma(df):\n",
    "    X_test  = vectorizer.transform(df.lyrics)\n",
    "    \n",
    "    df[\"Idioma\"] = NB.predict(X_test)\n",
    "    \n",
    "    return \"Exitoso Identificador de Idioma\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completado de palabras en español en caso de contracción\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 6: Levenshtein Metric__\n",
    "\n",
    "En español, como bien sabemos, no existen formalmente las contracciones; sin embargo, pragmáticamente se ha adquirido la costumbre de \"recortar\" algunas palabras y la forma de representar este fenómeno es por medio de un __'__. Comunmente estas contracciones se efectuan __en preposiciones__, esto es, stopwords. En cuanto a RI no son relevantes, pero para la interpretación literaria del texto, sí.\n",
    "\n",
    "Se pretende identificar estas palabras y completarlas por medio de una lista de palabras comunmente contraidas (informalmente) en el español. De no detectarse alguna candidata, es mejor elimina la palabra por cuestiones de carácteres especiales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contracciones_español(df,porcentaje = 60, lista_extra = []): \n",
    "    assert \"Spanish\" in df[\"Idioma\"].unique(), 'No escuchas música en español'\n",
    "    \n",
    "    lista = lista_extra + [palabra for palabra in stopwords.words(\"Spanish\") if len(palabra) >=3]\n",
    "    \n",
    "    español = df[df[\"Idioma\"] == \"Spanish\"][\"lyrics\"]\n",
    "    \n",
    "    for indice in español.index:\n",
    "            palabras_cancion = español[indice].split()\n",
    "            \n",
    "            for index in range(len(palabras_cancion)):\n",
    "                if \"'\" in palabras_cancion[index]:\n",
    "                    try:\n",
    "                        palabras_cancion[index] =  process.extractOne(palabras_cancion[index], lista,score_cutoff = porcentaje)[0]\n",
    "                    except:\n",
    "                        palabras_cancion[index] = \"\"\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            cancion_corregida = \" \".join(palabras_cancion)\n",
    "            \n",
    "            df.iloc[indice,1] = cancion_corregida\n",
    "    \n",
    "    return \"Exitosa corrección de palabras en la lista\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contracciones general(')\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 3: Regex (Expresiones regulares)__\n",
    "\n",
    "Dado que en algunas lenguas romances el uso de __'__ resulta determinante para el contexto de la oración, no puede ser fácilmente eliminado del corpus. En adición, las contracciones en el idioma inglés también existen y son muy comunes.  En general, en caso de que la contracción sea entre una preposición y una palabra relevante, es más probable que la palabra sea de mediana longitud. La función está primordialmente orientada a lenguas romance (incluyendo inglés) que las utilicen.\n",
    "\n",
    "Se pretende identificar contracciones útiles por medio de la identificación de la longitud de la segunda palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contracciones_general(df, limite = 4):\n",
    "    \n",
    "    \n",
    "    idiomas = df[~(df[\"Idioma\"] == \"Spanish\")][\"lyrics\"]\n",
    "    \n",
    "    for indice in idiomas.index:\n",
    "            palabras_cancion = idiomas[indice].split()\n",
    "            \n",
    "            for index in range(len(palabras_cancion)):\n",
    "                if \"'\" in palabras_cancion[index]:\n",
    "                    \n",
    "                    try:\n",
    "                        palabras_cancion[index] =  re.search(\"(?<=')\\w{4,}\",palabras_cancion[index])[0]\n",
    "                    except:\n",
    "                        palabras_cancion[index] = \"\"\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            cancion_corregida = \" \".join(palabras_cancion)\n",
    "            \n",
    "            df.iloc[indice,1] = cancion_corregida\n",
    "    \n",
    "    return \"Exitosas correcciones generales de contracciones en la lista\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizar Canciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizar(df):\n",
    "    df[\"tokens\"] = df[\"lyrics\"].apply(lambda x: set(nltk.word_tokenize(x))) # Tokenizar las canciones\n",
    "    \n",
    "    return \"Exitoso Tokenizado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar palabras de longitud mayor a..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eliminar_mayor_len(df,limite):\n",
    "    limpias = []\n",
    "    for i in range(df.shape[0]):\n",
    "        limpias.append({palabra_menos for palabra_menos in df.tokens[i] if len(palabra_menos) < limite})\n",
    "    \n",
    "    df[\"tokens\"] = limpias\n",
    "    \n",
    "    return f\"Exitosa eliminación de palabras con longitud mayor a {limite}\"\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar palabras de longitud menor a..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eliminar_menor_len(df,limite):\n",
    "    limpias = []\n",
    "    for i in range(df.shape[0]):\n",
    "        limpias.append({palabra_menos for palabra_menos in df.tokens[i] if len(palabra_menos) > limite})\n",
    "    \n",
    "    df[\"tokens\"] = limpias\n",
    "    \n",
    "    return f\"Exitosa eliminación de palabras con longitud menor a {limite}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stopwords(df, lista_extra = []):\n",
    "    # Lista de idiomas\n",
    "    idiomas = df.Idioma.unique()\n",
    "    \n",
    "    \n",
    "    for idioma in idiomas:\n",
    "        try:\n",
    "            stopwords_ = stopwords.words(idioma) + lista_extra # Stopwords\n",
    "        except:\n",
    "            print(\"No hay stopwords para\", idioma)\n",
    "            \n",
    "        canciones_idioma = df[df[\"Idioma\"] == idioma][\"tokens\"]\n",
    "        \n",
    "        for indice in canciones_idioma.index:\n",
    "            canciones_idioma[indice] = [palabra for palabra in canciones_idioma[indice] if palabra not in stopwords_]\n",
    "            \n",
    "        df.iloc[canciones_idioma.index,3] = canciones_idioma\n",
    "        \n",
    "    return \"Exitosa eliminación de stopwords por idiomas identificados\"\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordClouds por idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordClouds_idiomas(df):\n",
    "    \n",
    "    for idioma in df[\"Idioma\"].unique():\n",
    "    \n",
    "        freq = \" \".join(list(chain(*df[df[\"Idioma\"] == idioma][\"tokens\"])))\n",
    "        cloud = WordCloud(width = 8000,height = 8000, background_color = \"black\",max_words = 80).generate(freq)\n",
    "    \n",
    "        plt.figure(figsize=(25,15))\n",
    "        plt.imshow(cloud, interpolation = \"bilinear\")\n",
    "        plt.title(idioma, fontsize = 20, color = \"black\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmizar por idioma\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Stemming__\n",
    "\n",
    "\n",
    "La función solo está disponible para los siguientes idiomas:\n",
    "\n",
    "1. arabic\n",
    "2. danish\n",
    "3. dutch\n",
    "4. english\n",
    "5. finnish\n",
    "6. french\n",
    "7. german\n",
    "8. hungarian\n",
    "9. italian\n",
    "10. norwegian\n",
    "11. portuguese\n",
    "12. romanian\n",
    "13. russian\n",
    "14. spanish\n",
    "15. swedish\n",
    "\n",
    "De no estar disponible el idioma, simplemente se omite y se dejan los tokens originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_idiomas(df):\n",
    "    \n",
    "    df[\"tokens_stem\"] = np.nan\n",
    "    for idioma in df.Idioma.unique():\n",
    "        \n",
    "        try:\n",
    "            stemmer = SnowballStemmer(idioma.lower())\n",
    "            print(idioma, \" ¡disponible para stemmizar!\")\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        canciones_idioma = df[df[\"Idioma\"] == idioma][\"tokens\"]\n",
    "        \n",
    "        for indice in canciones_idioma.index:\n",
    "            canciones_idioma[indice] = [stemmer.stem(palabra) for palabra in canciones_idioma[indice]]\n",
    "            \n",
    "        df.iloc[canciones_idioma.index,4] = canciones_idioma\n",
    "        \n",
    "    return \"Exitosa eliminación de stopwords por idiomas identificados\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar palabras clave por (entre) canción(es)\n",
    "\n",
    "En este tópico utilizo:\n",
    "\n",
    "1. __Tema 8: TF-IDF__\n",
    "\n",
    "Se busca obtener una aproximación al contexto de las canciones por medio del TF-IDF. Se pretende encontrar canciones con contextos parecidos. Se separa por idiomas para una mejor visualización, aunque ciertamente podría no hacerse y aún con eso el algortimo seguiría funcionando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_mas_representativas_idioma(df, idioma):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(df[df[\"Idioma\"] == idioma].tokens_stem.apply(lambda x: \" \".join(x)).to_list())\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    tfidf = pd.DataFrame(denselist, columns=feature_names, index = df[df[\"Idioma\"] == idioma].name)\n",
    "    \n",
    "    lista = {cancion: tfidf.loc[cancion,tfidf.loc[cancion,] == tfidf.loc[cancion,].max()].to_dict() for cancion in tfidf.index}\n",
    "        \n",
    "        \n",
    "    \n",
    "    return lista\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Estadistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Obtener_datos(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English  ¡disponible para stemmizar!\n",
      "Spanish  ¡disponible para stemmizar!\n",
      "French  ¡disponible para stemmizar!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Exitosa eliminación de stopwords por idiomas identificados'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Identificar_Idioma(test)\n",
    "\n",
    "Contracciones_español(test)\n",
    "\n",
    "Contracciones_general(test)\n",
    "\n",
    "Tokenizar(test)\n",
    "\n",
    "Eliminar_mayor_len(test,10)\n",
    "\n",
    "Eliminar_menor_len(test,3)\n",
    "\n",
    "Stopwords(test,[\"yeah\",\"woah\",\"ohoh\",\"yeahyeah\",\"eeeeheheh\",\"oooohoooh\",\"woohoo\"])\n",
    "\n",
    "# WordClouds_idiomas(test)\n",
    "\n",
    "stemming_idiomas(test)\n",
    "\n",
    "#palabras_mas_representativas_idioma(test, \"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Star Treatment</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.767</td>\n",
       "      <td>7</td>\n",
       "      <td>-5.026</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fireside</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.953</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.611</td>\n",
       "      <td>0.0560</td>\n",
       "      <td>0.023400</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Veneno</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.664</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.872</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cuando Me Acerco A Ti</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.543</td>\n",
       "      <td>9</td>\n",
       "      <td>-9.037</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cornerstone</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.721</td>\n",
       "      <td>9</td>\n",
       "      <td>-5.810</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sur mes gardes</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.684</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ROXANNE</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.601</td>\n",
       "      <td>6</td>\n",
       "      <td>-5.616</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4600</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hola Señorita</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.945</td>\n",
       "      <td>0.2620</td>\n",
       "      <td>0.386000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11 PM</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.712</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.840</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>0.217000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Chained To The Rhythm</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.404</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1990</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>That's What Love Is</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.437</td>\n",
       "      <td>9</td>\n",
       "      <td>-6.959</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Swing</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.616</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.422</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ma musique</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.444</td>\n",
       "      <td>9</td>\n",
       "      <td>-8.618</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.722000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3830</td>\n",
       "      <td>0.329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nonstop</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.412</td>\n",
       "      <td>7</td>\n",
       "      <td>-8.074</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>positions</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.771</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hola - Remix</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.980</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Femme Fatal</td>\n",
       "      <td>0.827</td>\n",
       "      <td>0.620</td>\n",
       "      <td>10</td>\n",
       "      <td>-9.638</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.462000</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>No Buses</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.524</td>\n",
       "      <td>4</td>\n",
       "      <td>-9.051</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Otro Trago - Remix</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.331</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.071100</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Señorita</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.540</td>\n",
       "      <td>9</td>\n",
       "      <td>-6.039</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0945</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Vuelve</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.361</td>\n",
       "      <td>9</td>\n",
       "      <td>-9.053</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Four Out Of Five</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.310</td>\n",
       "      <td>0.0685</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Préféré (feat. OBOY)</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.467</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Toosie Slide</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.454</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.750</td>\n",
       "      <td>0.2010</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Watermelon Sugar</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0</td>\n",
       "      <td>-4.209</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Amor a Primera Vista</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.316</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.323000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.4050</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Dive</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.386</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.158</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.355000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>One Point Perspective</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.585</td>\n",
       "      <td>8</td>\n",
       "      <td>-4.570</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Copas de Vino</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.719</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.793</td>\n",
       "      <td>0.1680</td>\n",
       "      <td>0.492000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3360</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Carmín (feat. Juan Luis Guerra)</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.675</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.428</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Antes de Que Nos Olviden</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.598</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.024</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Everybody Wants To Rule The World</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.795</td>\n",
       "      <td>7</td>\n",
       "      <td>-12.095</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ADMV</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.390</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.959</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.817000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Tant pis</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.530</td>\n",
       "      <td>8</td>\n",
       "      <td>-6.592</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.471000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Come and Get Your Love</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.804</td>\n",
       "      <td>11</td>\n",
       "      <td>-7.621</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.0685</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>She</td>\n",
       "      <td>0.535</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.942</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Tus Besos</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.682</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.448</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.1210</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Monster (Shawn Mendes &amp; Justin Bieber)</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.383</td>\n",
       "      <td>2</td>\n",
       "      <td>-7.076</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Finesse</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.859</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.877</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Enfance 80</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.564</td>\n",
       "      <td>11</td>\n",
       "      <td>-8.543</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      name  danceability  energy  key  \\\n",
       "0                           Star Treatment         0.581   0.767    7   \n",
       "1                                 Fireside         0.538   0.953    9   \n",
       "2                                   Veneno         0.730   0.664    2   \n",
       "3                    Cuando Me Acerco A Ti         0.736   0.543    9   \n",
       "4                              Cornerstone         0.287   0.721    9   \n",
       "5                           Sur mes gardes         0.741   0.327    0   \n",
       "6                                  ROXANNE         0.621   0.601    6   \n",
       "7                            Hola Señorita         0.588   0.796    0   \n",
       "8                                    11 PM         0.777   0.712   10   \n",
       "9                    Chained To The Rhythm         0.562   0.800    0   \n",
       "10                     That's What Love Is         0.639   0.437    9   \n",
       "11                                   Swing         0.729   0.616    8   \n",
       "12                              Ma musique         0.520   0.444    9   \n",
       "13                                 Nonstop         0.912   0.412    7   \n",
       "14                               positions         0.737   0.802    0   \n",
       "15                            Hola - Remix         0.655   0.672    0   \n",
       "16                             Femme Fatal         0.827   0.620   10   \n",
       "17                                No Buses         0.481   0.524    4   \n",
       "18                      Otro Trago - Remix         0.725   0.790    0   \n",
       "19                                Señorita         0.759   0.540    9   \n",
       "20                                  Vuelve         0.746   0.361    9   \n",
       "21                        Four Out Of Five         0.601   0.863    0   \n",
       "22                    Préféré (feat. OBOY)         0.758   0.740    0   \n",
       "23                            Toosie Slide         0.834   0.454    1   \n",
       "24                        Watermelon Sugar         0.548   0.816    0   \n",
       "25                    Amor a Primera Vista         0.657   0.763    0   \n",
       "26                                    Dive         0.761   0.386    4   \n",
       "27                   One Point Perspective         0.714   0.585    8   \n",
       "28                           Copas de Vino         0.837   0.719    7   \n",
       "29         Carmín (feat. Juan Luis Guerra)         0.731   0.675   11   \n",
       "30                Antes de Que Nos Olviden         0.639   0.598    2   \n",
       "31       Everybody Wants To Rule The World         0.645   0.795    7   \n",
       "32                                    ADMV         0.565   0.390    9   \n",
       "33                                Tant pis         0.544   0.530    8   \n",
       "34                  Come and Get Your Love         0.731   0.804   11   \n",
       "35                                     She         0.535   0.521    0   \n",
       "36                               Tus Besos         0.682   0.682    9   \n",
       "37  Monster (Shawn Mendes & Justin Bieber)         0.652   0.383    2   \n",
       "38                                 Finesse         0.704   0.859    5   \n",
       "39                              Enfance 80         0.661   0.564   11   \n",
       "\n",
       "    loudness  speechiness  acousticness  instrumentalness  liveness  valence  \n",
       "0     -5.026       0.0527      0.243000          0.001310    0.1410    0.673  \n",
       "1     -5.611       0.0560      0.023400          0.001260    0.1130    0.740  \n",
       "2     -6.872       0.0346      0.372000          0.000000    0.2250    0.719  \n",
       "3     -9.037       0.2400      0.183000          0.000000    0.1970    0.245  \n",
       "4     -5.810       0.0387      0.007640          0.000000    0.3280    0.763  \n",
       "5     -8.684       0.0300      0.725000          0.000000    0.1010    0.590  \n",
       "6     -5.616       0.1480      0.052200          0.000000    0.4600    0.457  \n",
       "7     -3.945       0.2620      0.386000          0.000000    0.1980    0.546  \n",
       "8     -4.840       0.2770      0.217000          0.000000    0.0910    0.680  \n",
       "9     -5.404       0.1120      0.081400          0.000000    0.1990    0.471  \n",
       "10    -6.959       0.0355      0.692000          0.000000    0.1200    0.751  \n",
       "11    -6.422       0.2490      0.458000          0.000000    0.0820    0.532  \n",
       "12    -8.618       0.0359      0.722000          0.000000    0.3830    0.329  \n",
       "13    -8.074       0.1230      0.016500          0.012600    0.1040    0.423  \n",
       "14    -4.771       0.0878      0.468000          0.000000    0.0931    0.682  \n",
       "15    -5.980       0.3110      0.471000          0.000000    0.0767    0.343  \n",
       "16    -9.638       0.0302      0.225000          0.462000    0.1200    0.867  \n",
       "17    -9.051       0.0369      0.155000          0.000147    0.0523    0.425  \n",
       "18    -2.331       0.1970      0.071100          0.000001    0.0602    0.763  \n",
       "19    -6.039       0.0287      0.037000          0.000000    0.0945    0.750  \n",
       "20    -9.053       0.0528      0.369000          0.000000    0.0964    0.483  \n",
       "21    -4.310       0.0685      0.062600          0.000000    0.0717    0.676  \n",
       "22    -7.467       0.0925      0.017500          0.000000    0.0601    0.631  \n",
       "23    -9.750       0.2010      0.321000          0.000006    0.1140    0.837  \n",
       "24    -4.209       0.0465      0.122000          0.000000    0.3350    0.557  \n",
       "25    -6.316       0.0583      0.323000          0.000000    0.4050    0.950  \n",
       "26    -6.158       0.0399      0.355000          0.000000    0.0953    0.526  \n",
       "27    -4.570       0.0419      0.057000          0.004440    0.1440    0.842  \n",
       "28    -3.793       0.1680      0.492000          0.000000    0.3360    0.961  \n",
       "29    -5.428       0.0397      0.668000          0.000000    0.1600    0.753  \n",
       "30   -10.024       0.0302      0.417000          0.012900    0.1210    0.201  \n",
       "31   -12.095       0.0527      0.347000          0.003890    0.1040    0.535  \n",
       "32    -4.959       0.0556      0.817000          0.000000    0.1260    0.708  \n",
       "33    -6.592       0.0270      0.471000          0.000000    0.2600    0.629  \n",
       "34    -7.621       0.0473      0.172000          0.000280    0.0685    0.957  \n",
       "35    -5.942       0.0272      0.000532          0.371000    0.1900    0.457  \n",
       "36    -3.448       0.0979      0.165000          0.000004    0.1210    0.390  \n",
       "37    -7.076       0.0516      0.067600          0.000000    0.0828    0.549  \n",
       "38    -4.877       0.0996      0.018500          0.000000    0.0215    0.926  \n",
       "39    -8.543       0.0238      0.077000          0.000312    0.1120    0.327  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Obtener_datos_estadisticos(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crear lista de terminos relacionados en los idiomas seleccionados\n",
    "- Palabras relacionadas vectorialmente con las metricas spotify (despues estudiar palabras)\n",
    "- Documentar los 3 documentos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
